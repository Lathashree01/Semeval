{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Load data"
      ],
      "metadata": {
        "id": "9zqab4-cv9bR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we dont need this anymore as I have modified the file which also gives us processed data for test set\n",
        "# module_url = f\"https://raw.githubusercontent.com/Perez-AlmendrosC/dontpatronizeme/master/semeval-2022/dont_patronize_me.py\"\n",
        "# module_name = module_url.split('/')[-1]\n",
        "# print(f'Fetching {module_url}')\n",
        "# #with open(\"file_1.txt\") as f1, open(\"file_2.txt\") as f2\n",
        "# with request.urlopen(module_url) as f, open(module_name,'w') as outf:\n",
        "#   a = f.read()\n",
        "#   outf.write(a.decode('utf-8'))\n",
        "import os\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "class DontPatronizeMe:\n",
        "\n",
        "\tdef __init__(self, train_path, test_path):\n",
        "\n",
        "\t\tself.train_path = train_path\n",
        "\t\tself.test_path = test_path\n",
        "\t\tself.train_task1_df = None\n",
        "\t\tself.train_task2_df = None\n",
        "\t\tself.test_set_df = None\n",
        "\n",
        "\tdef load_task1(self):\n",
        "\t\t\"\"\"\n",
        "\t\tLoad task 1 training set and convert the tags into binary labels. \n",
        "\t\tParagraphs with original labels of 0 or 1 are considered to be negative examples of PCL and will have the label 0 = negative.\n",
        "\t\tParagraphs with original labels of 2, 3 or 4 are considered to be positive examples of PCL and will have the label 1 = positive.\n",
        "\t\tIt returns a pandas dataframe with paragraphs and labels.\n",
        "\t\t\"\"\"\n",
        "\t\trows=[]\n",
        "\t\twith open(os.path.join(self.train_path, 'dontpatronizeme_pcl.tsv')) as f:\n",
        "\t\t\tfor line in f.readlines()[4:]:\n",
        "\t\t\t\tpar_id=line.strip().split('\\t')[0]\n",
        "\t\t\t\tart_id = line.strip().split('\\t')[1]\n",
        "\t\t\t\tkeyword=line.strip().split('\\t')[2]\n",
        "\t\t\t\tcountry=line.strip().split('\\t')[3]\n",
        "\t\t\t\tt=line.strip().split('\\t')[4]#.lower()\n",
        "\t\t\t\tl=line.strip().split('\\t')[-1]\n",
        "\t\t\t\tif l=='0' or l=='1':\n",
        "\t\t\t\t\tlbin=0\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tlbin=1\n",
        "\t\t\t\trows.append(\n",
        "\t\t\t\t\t{'par_id':par_id,\n",
        "\t\t\t\t\t'art_id':art_id,\n",
        "\t\t\t\t\t'keyword':keyword,\n",
        "\t\t\t\t\t'country':country,\n",
        "\t\t\t\t\t'text':t, \n",
        "\t\t\t\t\t'label':lbin, \n",
        "\t\t\t\t\t'orig_label':l\n",
        "\t\t\t\t\t}\n",
        "\t\t\t\t\t)\n",
        "\t\tdf=pd.DataFrame(rows, columns=['par_id', 'art_id', 'keyword', 'country', 'text', 'label', 'orig_label']) \n",
        "\t\tself.train_task1_df = df\n",
        "\n",
        "\tdef load_task2(self, return_one_hot=True):\n",
        "\t\t# Reads the data for task 2 and present it as paragraphs with binarized labels (a list with seven positions, \"activated or not (1 or 0)\",\n",
        "\t\t# depending on wether the category is present in the paragraph).\n",
        "\t\t# It returns a pandas dataframe with paragraphs and list of binarized labels.\n",
        "\t\ttag2id = {\n",
        "\t\t\t\t'Unbalanced_power_relations':0,\n",
        "\t\t\t\t'Shallow_solution':1,\n",
        "\t\t\t\t'Presupposition':2,\n",
        "\t\t\t\t'Authority_voice':3,\n",
        "\t\t\t\t'Metaphors':4,\n",
        "\t\t\t\t'Compassion':5,\n",
        "\t\t\t\t'The_poorer_the_merrier':6\n",
        "\t\t\t\t}\n",
        "\t\tprint('Map of label to numerical label:')\n",
        "\t\tprint(tag2id)\n",
        "\t\tdata = defaultdict(list)\n",
        "\t\twith open (os.path.join(self.train_path, 'dontpatronizeme_categories.tsv')) as f:\n",
        "\t\t\tfor line in f.readlines()[4:]:\n",
        "\t\t\t\tpar_id=line.strip().split('\\t')[0]\n",
        "\t\t\t\tart_id = line.strip().split('\\t')[1]\n",
        "\t\t\t\ttext=line.split('\\t')[2]#.lower()\n",
        "\t\t\t\tkeyword=line.split('\\t')[3]\n",
        "\t\t\t\tcountry=line.split('\\t')[4]\n",
        "\t\t\t\tstart=line.split('\\t')[5]\n",
        "\t\t\t\tfinish=line.split('\\t')[6]\n",
        "\t\t\t\ttext_span=line.split('\\t')[7]\n",
        "\t\t\t\tlabel=line.strip().split('\\t')[-2]\n",
        "\t\t\t\tnum_annotators=line.strip().split('\\t')[-1]\n",
        "\t\t\t\tlabelid = tag2id[label]\n",
        "\t\t\t\tif not labelid in data[(par_id, art_id, text, keyword, country)]:\n",
        "\t\t\t\t\tdata[(par_id,art_id, text, keyword, country)].append(labelid)\n",
        "\n",
        "\t\tpar_ids=[]\n",
        "\t\tart_ids=[]\n",
        "\t\tpars=[]\n",
        "\t\tkeywords=[]\n",
        "\t\tcountries=[]\n",
        "\t\tlabels=[]\n",
        "\n",
        "\t\tfor par_id, art_id, par, kw, co in data.keys():\n",
        "\t\t\tpar_ids.append(par_id)\n",
        "\t\t\tart_ids.append(art_id)\n",
        "\t\t\tpars.append(par)\n",
        "\t\t\tkeywords.append(kw)\n",
        "\t\t\tcountries.append(co)\n",
        "\n",
        "\t\tfor label in data.values():\n",
        "\t\t\tlabels.append(label)\n",
        "\n",
        "\t\tif return_one_hot:\n",
        "\t\t\tlabels = MultiLabelBinarizer().fit_transform(labels)\n",
        "\t\tdf = pd.DataFrame(list(zip(par_ids, \n",
        "\t\t\t\t\t\t\t\t\tart_ids, \n",
        "\t\t\t\t\t\t\t\t\tpars, \n",
        "\t\t\t\t\t\t\t\t\tkeywords,\n",
        "\t\t\t\t\t\t\t\t\tcountries, \n",
        "\t\t\t\t\t\t\t\t\tlabels)), columns=['par_id',\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t'art_id', \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t'text', \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t'keyword',\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t'country', \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t'label',\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t])\n",
        "\t\tself.train_task2_df = df\n",
        "\n",
        "\n",
        "\tdef load_test(self):\n",
        "\t\t#self.test_df = [line.strip() for line in open(self.test_path)]\n",
        "\t\trows=[]\n",
        "\t\twith open(self.test_path) as f:\n",
        "\t\t\tfor line in f:\n",
        "\t\t\t\tt=line.strip().split('\\t')\n",
        "\t\t\t\trows.append(t)\n",
        "\t\tself.test_set_df = pd.DataFrame(rows, columns=\"par_id art_id keyword country text\".split())\n",
        "\t\t#drop the art_id,country columns\n",
        "\t\tself.test_set_df = self.test_set_df.drop(['art_id','country'], axis=1)\n",
        "\t\t#rename keyword to community\n",
        "\t\tself.test_set_df = self.test_set_df.rename(columns={'keyword':'community'})\n",
        "  \n",
        "\n",
        "\n",
        "# helper function to save predictions to an output file\n",
        "def labels2file(p, outf_path):\n",
        "\twith open(outf_path,'w') as outf:\n",
        "\t\tfor pi in p:\n",
        "\t\t\toutf.write(','.join([str(k) for k in pi])+'\\n')\n",
        "   \n",
        "\n",
        "dpm = DontPatronizeMe(r'', r'task4_test.tsv') # train and test paths\n",
        "dpm.load_task1() # load task 1 training set\n",
        "dpm.load_test() # load test set\n",
        "\n",
        "\n",
        "allData = dpm.train_task1_df # load training data\n",
        "\n",
        "########################################################\n",
        "\n",
        "trainIDs = pd.read_csv(r'train_semeval_parids-labels.csv')\n",
        "devIDS = pd.read_csv(r'dev_semeval_parids-labels.csv')\n",
        "trainIDs.par_id = trainIDs.par_id.astype(str)\n",
        "devIDS.par_id = devIDS.par_id.astype(str)\n",
        "\n",
        "# building the trainSet\n",
        "rows = [] # will contain par_id, label and text\n",
        "for idx in range(len(trainIDs)):  \n",
        "  parid = trainIDs.par_id[idx]\n",
        "  #print(parid)\n",
        "  # select row from original dataset to retrieve `text` and binary label\n",
        "  keyword = allData.loc[allData.par_id == parid].keyword.values[0]\n",
        "  text = allData.loc[allData.par_id == parid].text.values[0]\n",
        "  label = allData.loc[allData.par_id == parid].label.values[0]\n",
        "  rows.append({\n",
        "      'par_id':parid,\n",
        "      'community':keyword,\n",
        "      'text':text,\n",
        "      'label':label\n",
        "  })\n",
        "\n",
        "\n",
        "\n",
        "trainData = pd.DataFrame(rows)\n",
        "trainData.head()\n",
        "\n",
        "rows = [] # will contain par_id, label and text\n",
        "for idx in range(len(devIDS)):  \n",
        "  parid = devIDS.par_id[idx]\n",
        "  #print(parid)\n",
        "  # select row from original dataset\n",
        "  keyword = allData.loc[allData.par_id == parid].keyword.values[0]\n",
        "  text = allData.loc[allData.par_id == parid].text.values[0]\n",
        "  label = allData.loc[allData.par_id == parid].label.values[0]\n",
        "  rows.append({\n",
        "      'par_id':parid,\n",
        "      'community':keyword,\n",
        "      'text':text,\n",
        "      'label':label\n",
        "  })\n",
        "\n",
        "\n",
        "devData = pd.DataFrame(rows)\n",
        "devData.head()\n",
        "testData = dpm.test_set_df\n",
        "testData.head()"
      ],
      "metadata": {
        "id": "N4Ef7Soalu69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic regression"
      ],
      "metadata": {
        "id": "IB8wo4oAv1jx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53edEVJjlEtr",
        "outputId": "4279dd1f-8e1e-411f-c191-f9eac26e475c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LibLinear]0.2122448979591837\n",
            "0.1306532663316583\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "def extract_features(field,training_data,testing_data,type=\"tfidf\"):\n",
        "    \"\"\"Extract features using different methods\"\"\"\n",
        "    \n",
        "    \n",
        "    if \"binary\" in type:\n",
        "        \n",
        "        # BINARY FEATURE REPRESENTATION\n",
        "        cv= CountVectorizer(binary=True, max_df=0.7)\n",
        "        cv.fit_transform(training_data[field].values)\n",
        "        \n",
        "        train_feature_set=cv.transform(training_data[field])\n",
        "        test_feature_set=cv.transform(testing_data[field])\n",
        "        \n",
        "        return train_feature_set,test_feature_set,cv\n",
        "  \n",
        "    elif \"counts\" in type:\n",
        "        \n",
        "        # COUNT BASED FEATURE REPRESENTATION\n",
        "        cv= CountVectorizer(binary=False, max_df=0.7)\n",
        "        cv.fit_transform(training_data[field])\n",
        "        \n",
        "        train_feature_set=cv.transform(training_data[field])\n",
        "        test_feature_set=cv.transform(testing_data[field])\n",
        "        \n",
        "        return train_feature_set,test_feature_set,cv\n",
        "    \n",
        "    else:    \n",
        "      \n",
        "        # TF-IDF BASED FEATURE REPRESENTATION\n",
        "        tfidf_vectorizer=TfidfVectorizer(use_idf=True, max_df=0.7)\n",
        "        tfidf_vectorizer.fit_transform(training_data[field])\n",
        "        \n",
        "        train_feature_set=tfidf_vectorizer.transform(training_data[field])\n",
        "        test_feature_set=tfidf_vectorizer.transform(testing_data[field])   \n",
        "\n",
        "        return train_feature_set,test_feature_set,tfidf_vectorizer\n",
        "\n",
        "\n",
        "Y_train=trainData['label'].values\n",
        "Y_test=devData['label'].values\n",
        "X_train,X_test,feature_transformer=extract_features('text',trainData,devData)\n",
        "\n",
        "scikit_log_reg = LogisticRegression(verbose=1, solver='liblinear',random_state=0, C=5, penalty='l2',max_iter=400)\n",
        "model=scikit_log_reg.fit(X_train,Y_train)\n",
        "\n",
        "predictions=model.predict(X_test)\n",
        "\n",
        "print(f1_score(Y_test, predictions,pos_label=1, average='binary'))\n",
        "print(recall_score(Y_test, predictions, average='binary'))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hj7i_jlxmqJE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}